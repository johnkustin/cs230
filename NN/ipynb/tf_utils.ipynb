{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "#import h5py\n", 
        "import numpy as np\n", 
        "import tensorflow as tf\n", 
        "import math\n", 
        "\n", 
        "def load_dataset():\n", 
        "#    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n", 
        "#    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n", 
        "#    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n", 
        "#\n", 
        "#    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n", 
        "#    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n", 
        "#    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n", 
        "#\n", 
        "#    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n", 
        "#    \n", 
        "#    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n", 
        "#    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n", 
        "    train_set_x_orig = np.load('x_train.npy')\n", 
        "    train_set_y_orig = np.load('y_train.npy')\n", 
        "    test_set_x_orig = ()\n", 
        "    test_set_y_orig = ()\n", 
        "    classes = ()\n", 
        "\n", 
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n", 
        "\n", 
        "\n", 
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n", 
        "    \"\"\"\n", 
        "    Creates a list of random minibatches from (X, Y)\n", 
        "    \n", 
        "    Arguments:\n", 
        "    X -- input data, of shape (input size, number of examples)\n", 
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n", 
        "    mini_batch_size - size of the mini-batches, integer\n", 
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n", 
        "    \n", 
        "    Returns:\n", 
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n", 
        "    \"\"\"\n", 
        "    \n", 
        "    m = X.shape[1]                  # number of training examples\n", 
        "    mini_batches = []\n", 
        "    np.random.seed(seed)\n", 
        "    \n", 
        "    # Step 1: Shuffle (X, Y)\n", 
        "    permutation = list(np.random.permutation(m))\n", 
        "    shuffled_X = X[:, permutation]\n", 
        "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n", 
        "\n", 
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n", 
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n", 
        "    for k in range(0, num_complete_minibatches):\n", 
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n", 
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n", 
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n", 
        "        mini_batches.append(mini_batch)\n", 
        "    \n", 
        "    # Handling the end case (last mini-batch < mini_batch_size)\n", 
        "    if m % mini_batch_size != 0:\n", 
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n", 
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n", 
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n", 
        "        mini_batches.append(mini_batch)\n", 
        "    \n", 
        "    return mini_batches\n", 
        "\n", 
        "def convert_to_one_hot(Y, C):\n", 
        "    Y = np.eye(C)[Y.reshape(-1)].T\n", 
        "    return Y\n", 
        "\n", 
        "\n", 
        "def predict(X, parameters):\n", 
        "    \n", 
        "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n", 
        "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n", 
        "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n", 
        "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n", 
        "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n", 
        "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n", 
        "    \n", 
        "    params = {\"W1\": W1,\n", 
        "              \"b1\": b1,\n", 
        "              \"W2\": W2,\n", 
        "              \"b2\": b2,\n", 
        "              \"W3\": W3,\n", 
        "              \"b3\": b3}\n", 
        "    \n", 
        "    x = tf.placeholder(\"float\", [12288, 1])\n", 
        "    \n", 
        "    z3 = forward_propagation_for_predict(x, params)\n", 
        "    p = tf.argmax(z3)\n", 
        "    \n", 
        "    sess = tf.Session()\n", 
        "    prediction = sess.run(p, feed_dict = {x: X})\n", 
        "        \n", 
        "    return prediction\n", 
        "\n", 
        "def forward_propagation_for_predict(X, parameters):\n", 
        "    \"\"\"\n", 
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n", 
        "    \n", 
        "    Arguments:\n", 
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n", 
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n", 
        "                  the shapes are given in initialize_parameters\n", 
        "\n", 
        "    Returns:\n", 
        "    Z3 -- the output of the last LINEAR unit\n", 
        "    \"\"\"\n", 
        "    \n", 
        "    # Retrieve the parameters from the dictionary \"parameters\" \n", 
        "    W1 = parameters['W1']\n", 
        "    b1 = parameters['b1']\n", 
        "    W2 = parameters['W2']\n", 
        "    b2 = parameters['b2']\n", 
        "    W3 = parameters['W3']\n", 
        "    b3 = parameters['b3'] \n", 
        "                                                           # Numpy Equivalents:\n", 
        "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n", 
        "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n", 
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n", 
        "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n", 
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n", 
        "    \n", 
        "    return Z3\n", 
        "    \n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}